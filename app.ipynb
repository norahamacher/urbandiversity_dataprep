{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Superdiversity Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Preprocessing for the urban diversity web tool </h1>\n",
        "\n",
        "The urban superdiversity web tool requires a certain format of json to read data. You can create the format yourself or use the tools provided in this notebook. \n",
        "Below, find an example of the data used in the web app, with explaning comments. If you only wanted to know how the data is supposed to look like, you've found it and won't need the rest of this notebook!"
      ],
      "metadata": {
        "id": "Knhupb2U3E0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "{\"type\":\"FeatureCollection\",    //geojson format\n",
        "\"cityYear\":\"Vancouver-2006\",\n",
        "\"features\":[                    //each data point is in the features array. \n",
        "  {\"type\":\"Feature\",            //each data point is its own object, always type feature\n",
        "  \"properties\":{                //properties are copied from the shape file information\n",
        "    \"DAUID\":\"59150004\",\n",
        "    \"CSDUID\":\"5915055\",\n",
        "    \"CCSUID\":\"5915020\",\n",
        "    \"CDUID\":\"5915\",\n",
        "    \"ERUID\":\"5920\",\n",
        "    \"PRUID\":\"59\",\n",
        "    \"CTUID\":\"9330133.01\",\n",
        "    \"CMAUID\":\"933\",\"\n",
        "    indices\":{                  //indices is what was provided in the data csv file.\n",
        "      \"Population\":353,\n",
        "      \"Ethnicity-raw-count\":22,\n",
        "      \"Ethnicity-raw-normalized\":0.062323,\n",
        "      \"Mobility-raw-pct\":36.619718,\n",
        "      \"Generation-raw-SI\":0.732222,\n",
        "      \"Education-raw-SI\":0.796875,\n",
        "      \"Income-raw-SI\":0.865077}\n",
        "      },\n",
        "    \"geometry\":{               // point is the centroid of the polygon in the shapefile\n",
        "      \"type\":\"Point\",\n",
        "      \"coordinates\":[-123.25887520351195,49.38854863564009]},\n",
        "    \"geom_store\":{\n",
        "      \"type\":\"Polygon\",\n",
        "      \"coordinates\":[[[a,b],   //left out the proper coordinates, but this is an array of many lat/lon tuples\n",
        "  [c,d],\n",
        "  [...]]]}},\n",
        "  {...},\n",
        "  {...},\n",
        "  {...}]}                           this array of object contains all data points used on the map later.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "TAfOlmqk3xBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Data format needed for these scripts to work</h3>\n",
        "\n",
        "\n",
        "\n",
        "*   A shapefile, converted to geojson (use a program like ArcGIS to do this)\n",
        "*   A csv file containing all the parameters/indices you want to analyse. One column of this csv file MUST identify the matching parameter to the shape file. This is what the two files will be merged on.\n",
        "\n",
        "<h4> Step 1 </h4>\n",
        "Upload the two files here , by clicking on the upload icon below \"Files\" in the left menu. The scripts expect one file ending in .csv and one ending in .json.\n",
        "\n",
        "<h4> Step 2</h4>\n",
        "Provide the names for the matching parameter in both files. Please type the names in the form below, which have been populated with sample names. The csv_comparator should be the column name in your csv file which contains the list of entities, while the geojson_comparator should be the property within the file that matches the data in the csv column. \n",
        "You can also provide a meaningful name for your dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "VfLN0pI_5Uwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Names of matching parameters\n",
        "geojson_comparator = 'DAUID' #@param {type:\"string\"}\n",
        "csv_comparator = 'DA-ID' #@param {type:\"string\"}\n",
        "name = 'City-2700' #@param {type:\"string\"}\n",
        "\n"
      ],
      "metadata": {
        "id": "l01-jfqFH-tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4> Step 3 </h4>\n",
        "Execute the code cell below. If everything goes right, a new json will be saved in the same folder you uploaded your data to, which can be used in the MultiViz tool.\n",
        "\n",
        "<h5> Troubleshooting </h5>\n",
        "\n",
        "\n",
        "*   Code fails to execute: Please read the error message. The most likely culprits are: Could not find csv or json file.\n",
        "*   Resulting file does not show any of the indices in the csv! Please make sure you provided the correct spelling (case sensitive) for the column name in the csv for the geographical entity you want to  match and the same for the parameter in the shape file.\n",
        "*   Jenks calculation fails: Only numerical values work. \n"
      ],
      "metadata": {
        "id": "0hHUuS3RI7gq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytzo5OxVYL2i"
      },
      "source": [
        "from numpy.core import shape_base\n",
        "#some required libraries for the conversion.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "!pip install jenkspy\n",
        "import jenkspy\n",
        "import json\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "from shapely.geometry import Polygon\n",
        "\n",
        "#read the csv\n",
        "for filename in Path(\".\").glob(\"*.csv\"):\n",
        "  try:\n",
        "    df_indices = pd.read_csv(filename, index_col=False)\n",
        "  except IOError:\n",
        "    print(\"Error loading csv. Please upload one csv file and one geojson file to the main directory of this script.\") \n",
        "\n",
        "#read the geojson (sometimes only json ending)\n",
        "for filename in Path(\".\").glob(\"*json\"):\n",
        "  try:\n",
        "    shape = json.load(open(filename))\n",
        "  except IOError:\n",
        "    print(\"Error while loading geojson. Please upload one csv file and one geojson file to the main directory of this script.\") \n",
        "\n",
        "try:\n",
        "  print(\"Successfully loaded csv. \")\n",
        "  #print(df_indices)\n",
        "except BaseException:\n",
        "  print(\"Could not find csv data.\")\n",
        "\n",
        "try:\n",
        "  df_shape = pd.DataFrame(shape[\"features\"])\n",
        "  print(\"Successfully loaded geojson. \")\n",
        "  #print(df_shape)\n",
        "except BaseException:\n",
        "  print(\"Could not find geojson data.\")\n",
        "\n",
        "#converts the value to numeric (int if int, else float). \n",
        "#returns -1 if:\n",
        "#   a) the value is not numeric, check for strings reflecting numeric values is implemented\n",
        "#   b) if the value is nan\n",
        "def makeFloat(val):\n",
        "\n",
        "  if(isinstance(val,float)):\n",
        "    if(math.isnan(val)):\n",
        "      return -1\n",
        "    else:\n",
        "      return val\n",
        "  elif (isinstance(val,int)):\n",
        "    if(math.isnan(val)):\n",
        "      return -1\n",
        "    else:\n",
        "      return val\n",
        "  else: #string\n",
        "    try:\n",
        "      res = float(val)\n",
        "      if(math.isnan(res)):\n",
        "        return -1\n",
        "      else:\n",
        "        return res\n",
        "    except ValueError:  #casting not possible --> not a numeric value in the string\n",
        "      return -1\n",
        "\n",
        "#Merging code. Grabs the row from the csv that matches the geojson_comparator from each entry in the geojson and puts the indices in a new  indices property.\n",
        "for prop in df_shape[\"properties\"]:  \n",
        "  prop[\"indices\"] = {}  #the indices from the csv file will be put in this indices property, when the comparators match.\n",
        "\n",
        "  \n",
        "  row = df_indices.loc[df_indices[csv_comparator]==prop[geojson_comparator]]  #find the correct row\n",
        "\n",
        "  rowDict = row.to_dict(orient='records')\n",
        "\n",
        "  if len(rowDict)>0:\n",
        "    for key in rowDict[0]:  #go through the keys and add them to the json object.\n",
        "      if (key.find(\"ndex\") < 0) & (key!=csv_comparator):\n",
        "            #print(row[key])\n",
        "          val = makeFloat(row[key])  #TODO: This needs to change if we're ever going to use non-numerical values in our evaluations. \n",
        "          prop[\"indices\"][key] = round(val,6)\n",
        "\n",
        "\n",
        "#create the new resulting json.\n",
        "newJson = {}\n",
        "newJson[\"type\"] = \"FeatureCollection\"\n",
        "newJson[\"cityYear\"] = name\n",
        "newJson[\"features\"] = []\n",
        "\n",
        "#write the data to the new geojson object. Also adds centroid property for the geometry\n",
        "for index in df_shape.index:\n",
        "  obj = {\"type\": \"Feature\", \"properties\": df_shape[\"properties\"][index],\"geometry\":df_shape[\"geometry\"][index]}\n",
        "   #create the centroid for the data points\n",
        "  geometry = obj[\"geometry\"]\n",
        "  #print(geometry)\n",
        "  if (type( geometry['coordinates'][0][0][0] ) is float):\n",
        "    P = Polygon(geometry['coordinates'][0])\n",
        "  else :\n",
        "    P = Polygon(geometry['coordinates'][0][0])\n",
        "  circle = P.centroid\n",
        "\n",
        "  geometry_ = {\"type\":\"Point\",\"coordinates\":[circle.x,circle.y]}\n",
        "  obj[\"geometry\"]=geometry_\n",
        "  obj[\"geom_store\"] = geometry\n",
        "  newJson[\"features\"].append(obj)\n",
        "\n",
        "df_shape = newJson\n",
        "\n",
        "with open(\"./\"+ name +'.json', 'w') as f: #dump the data in the new file.\n",
        "    f.write(json.dumps(df_shape, separators=(',', ':')))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4> Jenks values </h4>\n",
        "If you want to add jenks values to your data, execute the cell below. This only works with numerical values per parameter. This will create a new file, named the same but with _withjenks suffix."
      ],
      "metadata": {
        "id": "nKXn4dRoZBR4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16NYEDMZYkmq"
      },
      "source": [
        "#mediator function to call jenkspy jenks calculation\n",
        "def getJenks(arr,num):\n",
        "  return jenkspy.jenks_breaks(arr, nb_class=num)\n",
        "\n",
        "#get the jenks values from the jenksVals, which have been aggregated per parameters.\n",
        "def makeJenks(data, jenksVals):\n",
        "  currJson={}\n",
        "  range = [12] #Jenks steps to be used\n",
        "  print(data.keys())\n",
        "  currJson[\"all\"]={}\n",
        "  for key in jenksVals.keys():\n",
        "    currJson[\"all\"][key] = {} \n",
        "    for val in range:\n",
        "      currJson[\"all\"][key][val] = getJenks(jenksVals[key], val)  \n",
        "\n",
        "  data[\"jenks\"] = currJson  #attach the jenks values to the data.\n",
        "  #print(data.keys())\n",
        "  with open(\"./\"+ name +'_withjenks.json', 'w') as f: #dump the data in the new file.\n",
        "    f.write(json.dumps(data, separators=(',', ':')))\n",
        "\n",
        "\n",
        "arr = {\"name\":name, \"all\": {}}\n",
        "#aggregate all values for all parameters in an array per parameter, which will be used for the jenks calculation.\n",
        "#in the same loop, we also caluclate the centroid for each polygon which is used for the bars in the multiviz tool.\n",
        "for feat in df_shape[\"features\"]:\n",
        "\n",
        "  for key in feat['properties']['indices'].keys(): #go through all the indices\n",
        "      val = feat['properties']['indices'][key]\n",
        "\n",
        "      if (key in arr['all'].keys()):  #is this index alread in the result? then add to that key\n",
        "        if (val >=0 ): \n",
        "          arr['all'][key].append(val)\n",
        "      else:\n",
        "        if (val >=0 ) :\n",
        "            arr['all'][key] = []\n",
        "            arr['all'][key].append(val)\n",
        "\n",
        "makeJenks(df_shape,arr[\"all\"]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFjfOw2RYG3J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44f1dc14-6dec-4196-d89f-940cc8ed13a4"
      },
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['Total-pop', 'Generation-raw-SI', 'Income-raw-SI', 'Ethnicity-raw-count', 'Ethnicity-raw-norm', 'Education-raw-SI', 'Mobility-raw-pct_old', 'Mobility-raw-pct'])\n",
            "dict_keys(['type', 'cityYear', 'features'])\n"
          ]
        }
      ]
    }
  ]
}